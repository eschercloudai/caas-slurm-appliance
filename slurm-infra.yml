---

# Provision the infrastructure using Terraform
- name: Provision infrastructure
  hosts: openstack
  pre_tasks:
    # We need to convert the floating IP id to an address for Terraform
    - name: Look up floating IP
      include_role:
        name: stackhpc.terraform-infra
        tasks_from: lookup_floating_ip
      vars:
        os_floating_ip_id: "{{ cluster_floating_ip }}"
  roles:
    - role: stackhpc.terraform-infra
      vars:
        # Image cloud user, use extraVars.__ALL__.cluster_user in values.yaml
        # to change if required
        cluster_ssh_user: "{{ cluster_user | default('rocky') }}"
        # Variables controlling the Terraform provisioning
        terraform_project_path: "{{ playbook_dir }}/terraform"
        terraform_state: "{{ cluster_state | default('present') }}"
        terraform_backend_type: "{{ ( 'CONSUL_HTTP_ADDR' in ansible_env ) | ternary('consul', 'local') }}"
        terraform_backend_config: "{{ terraform_backend_config_defaults[terraform_backend_type] }}"
        terraform_variables:
          cluster_name: "{{ cluster_name }}"
          cluster_network: "{{ cluster_network }}"
          cluster_floating_ip: "{{ os_floating_ip_info.floating_ip_address }}"
          # We need to inject the user's SSH key and the deploy key for the cluster
          cluster_ssh_public_keys: "{{ [cluster_deploy_ssh_public_key, cluster_user_ssh_public_key] | to_json }}"
          compute_count: "{{ compute_count }}"
          login_image: "{{ cluster_image }}"
          control_image: "{{ cluster_image }}"
          compute_image: "{{ cluster_image }}"
          login_flavor: "{{ login_flavor }}"
          control_flavor: "{{ control_flavor }}"
          compute_flavor: "{{ compute_flavor }}"
  # The hosts provisioned by Terraform are put into a primary group by the role
  # These tasks then add those hosts to additional groups depending on the selected options
  post_tasks:
    # Add the hosts to the required groups
    - name: Add cluster hosts to required groups
      add_host:
        name: "{{ item }}"
        groups: "{{ hostvars[item].group_names | terraform_infra_expand_groups(cluster_groups_required) }}"
      loop: "{{ groups.get('cluster', []) }}"
    # If monitoring was requested, add the monitoring groups to hosts
    - name: Add cluster hosts to monitoring groups
      add_host:
        name: "{{ item }}"
        groups: "{{ hostvars[item].group_names | terraform_infra_expand_groups(cluster_groups_monitoring) }}"
      loop: "{{ groups.get('cluster', []) }}"
      when: cluster_monitoring_enabled | default(false) | bool
    # If validation was requested, add the validation groups to hosts
    - name: Add cluster hosts to validation groups
      add_host:
        name: "{{ item }}"
        groups: "{{ hostvars[item].group_names | terraform_infra_expand_groups(cluster_groups_validation) }}"
      loop: "{{ groups.get('cluster', []) }}"
      when: cluster_run_validation | default(false) | bool
    # If Zenith is enabled, add the zenith groups to hosts
    - name: Add clusters hosts to Zenith groups
      add_host:
        name: "{{ item }}"
        groups: "{{ hostvars[item].group_names | terraform_infra_expand_groups(cluster_groups_zenith) }}"
      loop: "{{ groups.get('cluster', []) }}"
      when: zenith_subdomain_monitoring is defined

# Ensure that the hosts in the cluster can all refer to each other by their hostname
- hosts: cluster
  become: yes
  tasks:
    - name: Populate /etc/hosts with cluster hosts
      lineinfile:
        path: /etc/hosts
        regexp: "{{ hostvars[host].inventory_hostname }}"
        line: "{{ hostvars[host].ansible_default_ipv4.address }} {{ hostvars[host].inventory_hostname }}"
      loop: "{{ ansible_play_hosts }}"
      loop_control:
        loop_var: host

# Ensure that the secrets are generated and persisted on the control host
- name: Generate and persist secrets
  hosts: control
  gather_facts: no
  become: yes
  roles:
    - persist_openhpc_secrets

# Configure the hosts as a Slurm cluster
# Use the playbooks invidually rather than the site playbook as it avoids the
# need to define the environment variables referencing an environment

# validate.yml asserts presence of a control group which doesn't exist when 
# destroying infra, so only validate when we're not destroying
- import_playbook: vendor/stackhpc/ansible-slurm-appliance/ansible/validate.yml
  when: cluster_state is not defined or (cluster_state is defined and cluster_state != "absent")

# The first task in the bootstrap playbook causes the home directory of the centos user to be moved
# on the first run
# This can disrupt the SSH connection, particularly because we use the login host as a jump host
# So we move the home directory on the login node and reset the connection first
- hosts: login
  gather_facts: false
  tasks:
    - name: Set up Ansible user
      user: "{{ appliances_local_users_ansible_user }}"
      become_method: "sudo"
      # Need to change working directory otherwise we try to switch back to non-existent directory.
      become_flags: '-i'
      become: true

- hosts: cluster
  gather_facts: no
  tasks:
    - name: Reset persistent SSH connections
      meta: reset_connection

- import_playbook: vendor/stackhpc/ansible-slurm-appliance/ansible/bootstrap.yml
- import_playbook: vendor/stackhpc/ansible-slurm-appliance/ansible/filesystems.yml
- import_playbook: vendor/stackhpc/ansible-slurm-appliance/ansible/slurm.yml

# Deploy the monitoring stack first
- import_playbook: vendor/stackhpc/ansible-slurm-appliance/ansible/monitoring.yml

# Configure the Zenith clients that are required
# First, ensure that podman is installed on all hosts that will run Zenith clients
- hosts: zenith,!podman
  tasks:
    - import_role:
        name: podman
        tasks_from: prereqs.yml
    - import_role:
        name: podman
        tasks_from: config.yml
# Deploy the Zenith client for Grafana
- hosts: grafana
  tasks:
    - include_role:
        name: zenith_proxy
      vars:
        zenith_proxy_service_name: zenith-monitoring
        zenith_proxy_upstream_host: "{{ grafana_address }}"
        zenith_proxy_upstream_port: "{{ grafana_port }}"
        zenith_proxy_client_token: "{{ zenith_token_monitoring }}"
        zenith_proxy_client_auth_params: {}
      when: zenith_subdomain_monitoring is defined

- import_playbook: vendor/stackhpc/ansible-slurm-appliance/ansible/adhoc/hpctests.yml

# Write the outputs as the final task
- hosts: openstack
  tasks:
    - debug: var=outputs
      vars:
        outputs:
          cluster_access_ip: "{{ os_floating_ip_info.floating_ip_address }}"
